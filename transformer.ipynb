{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_size % heads == 0, \"Embedding size needs to be divisible by heads\"\n",
    "        self.heads = heads\n",
    "        self.embed_size = embed_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split embedding into self.heads different pieces\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (N, heads, query_len, key_len)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.embed_size\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden)\n",
    "        self.fc2 = nn.Linear(ff_hidden, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = FeedForward(embed_size, ff_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, embed_size)\n",
    "        self.encoding.requires_grad = False  # We don't want to update positional encoding during training\n",
    "\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        _2i = torch.arange(0, embed_size, step=2)\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / embed_size)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / embed_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:seq_len, :].to(x.device)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden, num_layers, vocab_size, max_len=100, dropout=0.5):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position = PositionalEncoding(embed_size, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, heads, ff_hidden, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x, x, x, mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (word_embedding): Embedding(2, 6)\n",
      "  (position): PositionalEncoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (attention): MultiHeadSelfAttention(\n",
      "        (values): Linear(in_features=3, out_features=6, bias=False)\n",
      "        (keys): Linear(in_features=3, out_features=6, bias=False)\n",
      "        (queries): Linear(in_features=3, out_features=6, bias=False)\n",
      "        (fc_out): Linear(in_features=6, out_features=6, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (fc1): Linear(in_features=6, out_features=24, bias=True)\n",
      "        (fc2): Linear(in_features=24, out_features=6, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=6, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([1, 16, 6])\n",
      "torch.Size([1, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the Transformer\n",
    "embed_size = 6           # Embedding dimension for each token\n",
    "heads = 2                # Number of attention heads (embedding size should be divisible by this)\n",
    "ff_hidden = 24           # Dimension of the feed-forward layer (you can adjust this)\n",
    "num_layers = 4           # Number of transformer layers\n",
    "vocab_size = 2       # Vocabulary size (can be any appropriate number for your use case)\n",
    "max_len = 100            # Maximum sequence length (100 tokens in your case)\n",
    "dropout = 0.1            # Dropout rate for regularization\n",
    "\n",
    "# Create a Transformer object\n",
    "transformer = Transformer(\n",
    "    embed_size=embed_size,\n",
    "    heads=heads,\n",
    "    ff_hidden=ff_hidden,\n",
    "    num_layers=num_layers,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    dropout=dropout\n",
    ").cuda()\n",
    "\n",
    "# Print the model summary (or model architecture)\n",
    "print(transformer)\n",
    "\n",
    "\n",
    "# Create a random input tensor with token indices between 0 and vocab_size-1\n",
    "random_input = torch.rand( 1, 16, 6).float().cuda()\n",
    "\n",
    "# Print the shape of the input tensor\n",
    "print(random_input.shape)  # Should be: (32, 100, 6)\n",
    "\n",
    "# Example of passing the random input through the transformer model\n",
    "mask = None  # No mask applied here, but can be added if needed\n",
    "output = transformer(random_input, mask)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)  # Output should be: (32, 100, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor, FlattenExtractor\n",
    "from stable_baselines3.common.distributions import (\n",
    "    DiagGaussianDistribution,\n",
    "    CategoricalDistribution,\n",
    "    MultiCategoricalDistribution,\n",
    "    BernoulliDistribution,\n",
    "    StateDependentNoiseDistribution,\n",
    "    make_proba_distribution,\n",
    ")\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import warnings\n",
    "import collections\n",
    "\n",
    "# Transformer Components\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_size % heads == 0, \"Embedding size needs to be divisible by heads\"\n",
    "        self.heads = heads\n",
    "        self.embed_size = embed_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden)\n",
    "        self.fc2 = nn.Linear(ff_hidden, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = FeedForward(embed_size, ff_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, embed_size)\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        _2i = torch.arange(0, embed_size, step=2)\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / embed_size)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / embed_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:seq_len, :].to(x.device)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden, num_layers, vocab_size, max_len=100, dropout=0.5):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position = PositionalEncoding(embed_size, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, heads, ff_hidden, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x, x, x, mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ActorCriticPolicy with Transformer\n",
    "\n",
    "class ActorCriticTransformerPolicy(BasePolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[torch.optim.Optimizer] = torch.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        embed_size: int = 128,\n",
    "        heads: int = 8,\n",
    "        ff_hidden: int = 512,\n",
    "        num_layers: int = 3,\n",
    "        vocab_size: int = 100,\n",
    "        max_len: int = 100,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            squash_output=squash_output,\n",
    "            normalize_images=normalize_images,\n",
    "        )\n",
    "\n",
    "        # Initialize the Transformer policy and value networks\n",
    "        self.transformer = Transformer(embed_size, heads, ff_hidden, num_layers, vocab_size, max_len, dropout)\n",
    "\n",
    "        # Initialize action distribution\n",
    "        self.action_dist = make_proba_distribution(action_space, use_sde=use_sde, dist_kwargs=None)\n",
    "\n",
    "        self.optimizer = optimizer_class(self.parameters(), lr=lr_schedule(1), **optimizer_kwargs)\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, mask=None, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        features = self.extract_features(obs)\n",
    "        latent = self.transformer(features, mask)\n",
    "\n",
    "        # Split into policy and value latent vectors\n",
    "        latent_pi, latent_vf = latent, latent\n",
    "\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        actions = distribution.get_actions(deterministic=deterministic)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "\n",
    "        actions = actions.reshape((-1, *self.action_space.shape))\n",
    "        return actions, values, log_prob\n",
    "\n",
    "    def extract_features(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return obs  # You might want to replace this with a feature extractor if needed\n",
    "\n",
    "    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> torch.distributions.Distribution:\n",
    "        mean_actions = self.action_net(latent_pi)\n",
    "        return self.action_dist.proba_distribution(mean_actions, self.log_std)\n",
    "\n",
    "    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.extract_features(obs)\n",
    "        latent_vf = self.transformer(features, None)\n",
    "        return self.value_net(latent_vf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
